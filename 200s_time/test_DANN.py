import os
import numpy as np
import tensorflow as tf
import pickle
from datetime import datetime
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Dropout, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from NuRadioReco.utilities import units

from A0_Utilities import load_sim_rcr, load_data
from refactor_train_and_run import load_and_prep_data_for_training

# --- Configuration ---
def get_config():
    amp = '200s'
    config = {
        'amp': amp,
        'output_cut_value': 0.6,
        'train_cut': 4000,
        'noise_rms_200s': 22.53 * units.mV,
        'noise_rms_100s': 20 * units.mV,
        'station_ids_200s': [14, 17, 19, 30],
        'station_ids_100s': [13, 15, 18],
        'sim_date': '5.28.25',
        'base_sim_folder': '/dfs8/sbarwick_lab/ariannaproject/rricesmi/simulatedRCRs/',
        'base_model_path': '/pub/tangch3/ARIANNA/DeepLearning/refactor/models/',
        'base_plot_path': '/pub/tangch3/ARIANNA/DeepLearning/refactor/plots/',
        'loading_data_type': 'new_chi_above_curve',
        'model_filename_template': '{timestamp}_{amp}_RCR_Backlobe_model_2Layer.h5',
        'history_filename_template': '{timestamp}_{amp}_RCR_Backlobe_model_2Layer_history.pkl',
        'loss_plot_filename_template': '{timestamp}_{amp}_{prefix}_loss_plot_RCR_Backlobe_model_2Layer.png',
        'accuracy_plot_filename_template': '{timestamp}_{amp}_{prefix}_accuracy_plot_RCR_Backlobe_model_2Layer.png',
        'tsne_plot_filename_template': '{timestamp}_{amp}_tsne_feature_space_RCR_Backlobe_model_2Layer.png',
        'histogram_filename_template': '{timestamp}_{amp}_train_and_run_histogram.png',
        'early_stopping_patience': 5,
        'keras_epochs': 50,
        'keras_batch_size': 64,
        'verbose_fit': 2,
        'lambda_adversary': 1.0,
        'lambda_schedule': [(0, 0.0), (10, 0.1), (20, 0.5), (30, 1.0)],
        'input_shape': (4, 256, 1),
    }
    config['noise_rms'] = config['noise_rms_200s'] if amp == '200s' else config['noise_rms_100s']
    config['station_ids'] = config['station_ids_200s'] if amp == '200s' else config['station_ids_100s']
    return config

# --- Gradient Reversal Layer ---
class ScheduledGradientReversal(tf.keras.layers.Layer):
    def __init__(self, schedule, **kwargs):
        super().__init__(**kwargs)
        self.schedule = schedule
        self.global_step = tf.Variable(0, trainable=False, dtype=tf.int32)

    def call(self, x):
        lambda_val = self.get_lambda()

        @tf.custom_gradient
        def _flip_gradient(x):
            def grad(dy):
                return -lambda_val * dy
            return x, grad

        return _flip_gradient(x)

    def get_lambda(self):
        lambda_val = 0.0
        for step, val in self.schedule:
            if self.global_step >= step:
                lambda_val = val
            else:
                break
        return tf.convert_to_tensor(lambda_val, dtype=tf.float32)

    def increment_step(self):
        self.global_step.assign_add(1)

# --- DANN Model ---
def build_dann_model(input_shape, lambda_schedule):
    inputs = Input(shape=input_shape)
    x = Conv2D(16, (4, 10), activation='relu')(inputs)
    x = Conv2D(8, (1, 10), activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Flatten(name='flatten')(x)
    label_output = Dense(1, activation='sigmoid', name='label_output')(x)
    reversal_layer = ScheduledGradientReversal(schedule=lambda_schedule, name='gradient_reversal')
    reversed_x = reversal_layer(x)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(reversed_x)
    model = Model(inputs=inputs, outputs=[label_output, domain_output])
    model.reversal_layer = reversal_layer
    model.compile(
        optimizer=Adam(learning_rate=1e-4, clipvalue=1.0),
        loss={'label_output': 'binary_crossentropy', 'domain_output': 'binary_crossentropy'},
        loss_weights={'label_output': 1.0, 'domain_output': 1.0},
        metrics={'label_output': 'accuracy', 'domain_output': 'accuracy'}
    )
    return model

# --- Prepare Data ---
def prep_dann_data(config):
    data = load_and_prep_data_for_training(config)
    x_rcr = data['training_rcr']
    x_bl = data['training_backlobe']
    y_rcr = np.ones(len(x_rcr))
    y_bl = np.zeros(len(x_bl))
    x_source = np.concatenate([x_rcr, x_bl])
    y_source = np.concatenate([y_rcr, y_bl])
    x_target = data['data_backlobe_tracesRCR_all']
    x_domain = np.concatenate([x_source, x_target])
    y_domain = np.concatenate([
        np.zeros(len(x_source)),
        np.ones(len(x_target))
    ])
    x_source = x_source[..., np.newaxis]
    x_domain = x_domain[..., np.newaxis]
    return x_source, y_source, x_domain, y_domain

# --- t-SNE Visualization ---
from sklearn.manifold import TSNE

def plot_tsne_features(model, x_source, x_target, config):
    feature_model = Model(inputs=model.input, outputs=model.get_layer('flatten').output)
    features_src = feature_model.predict(x_source, batch_size=128)
    features_tgt = feature_model.predict(x_target, batch_size=128)
    features = np.concatenate([features_src, features_tgt])
    labels = np.array([0] * len(features_src) + [1] * len(features_tgt))
    tsne = TSNE(n_components=2, random_state=42)
    features_2d = tsne.fit_transform(features)
    plt.figure(figsize=(8, 6))
    plt.scatter(features_2d[labels == 0, 0], features_2d[labels == 0, 1], label='Source (Sim+BL)', alpha=0.6)
    plt.scatter(features_2d[labels == 1, 0], features_2d[labels == 1, 1], label='Target (All Station Data)', alpha=0.6)
    plt.title('t-SNE of Feature Representations')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join('/pub/tangch3/ARIANNA/DeepLearning/refactor/tests/', config['tsne_plot_filename_template'].format(timestamp=timestamp, amp=config['amp'])))
    plt.close()

# --- Training History Plotting ---
def plot_dann_training_history(history, config, amp, timestamp, output_dir='.'): 
    hist = history.history
    def plot_metric(metric, val_metric, title, ylabel, filename):
        plt.figure(figsize=(8, 6))
        plt.plot(hist[metric], label='Train')
        plt.plot(hist[val_metric], label='Validation')
        plt.xlabel('Epoch')
        plt.ylabel(ylabel)
        plt.title(title)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, filename))
        plt.close()
    plot_metric('label_output_loss', 'val_label_output_loss', 'Label Loss (RCR task)', 'Loss',
                config['loss_plot_filename_template'].format(timestamp=timestamp, amp=amp, prefix='label'))
    plot_metric('label_output_accuracy', 'val_label_output_accuracy', 'Label Accuracy (RCR task)', 'Accuracy',
                config['accuracy_plot_filename_template'].format(timestamp=timestamp, amp=amp, prefix='label'))
    plot_metric('domain_output_loss', 'val_domain_output_loss', 'Domain Loss (Domain Discriminator)', 'Loss',
                config['loss_plot_filename_template'].format(timestamp=timestamp, amp=amp, prefix='domain'))
    plot_metric('domain_output_accuracy', 'val_domain_output_accuracy', 'Domain Accuracy (should â†’ 50%)', 'Accuracy',
                config['accuracy_plot_filename_template'].format(timestamp=timestamp, amp=amp, prefix='domain'))

# --- Main ---
if __name__ == '__main__':
    cfg = get_config()
    x_source, y_source, x_domain, y_domain = prep_dann_data(cfg)
    n_target = len(x_domain) - len(x_source)
    y_target_dummy = np.zeros(n_target)
    y_combined = np.concatenate([y_source, y_target_dummy])
    y_domain = y_domain.reshape(-1, 1)
    y_combined = y_combined.reshape(-1, 1)

    model = build_dann_model(input_shape=cfg['input_shape'], lambda_schedule=cfg['lambda_schedule'])
    model.summary()

    callbacks = [
        EarlyStopping(patience=cfg['early_stopping_patience'], restore_best_weights=True)
    ]

    history = model.fit(
        x=x_domain,
        y={'label_output': y_combined, 'domain_output': y_domain},
        epochs=cfg['keras_epochs'],
        batch_size=cfg['keras_batch_size'],
        validation_split=0.2,
        callbacks=callbacks,
        verbose=cfg['verbose_fit'],
        shuffle=True
    )

    # manually increment step after each epoch
    for _ in range(cfg['keras_epochs']):
        model.reversal_layer.increment_step()

    timestamp = datetime.now().strftime('%m.%d.%y_%H-%M')
    model_filename = cfg['model_filename_template'].format(timestamp=timestamp, amp=cfg['amp'])
    model_path = os.path.join(cfg['base_model_path'], model_filename)
    model.save(model_path)
    print(f'Model saved to: {model_path}')

    x_target = x_domain[len(x_source):]
    plot_tsne_features(model, x_source, x_target, config=cfg)

    plot_dann_training_history(history, config=cfg, amp=cfg['amp'], timestamp=timestamp,
                               output_dir='/pub/tangch3/ARIANNA/DeepLearning/refactor/tests/')
